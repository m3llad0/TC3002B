Natural language processing is a discipline rooted in both linguistics and computer science. It incorporates syntactic problems (grammatical category, word segmentation, name entity recognition, etc.), semantics (sentiments analysis, texts categorization, translation, questions answering, etc.), vocal signals generation from texts or texts generation from vocal signals, etc. In the last few years, some scientists and companies have been able to create algorithms capable of achieving very high levels of performance for some of these tasks such as translation or sentiment classification, in part, by using big data. The fact that some algorithms perform so well with such a large amount of data gives a significant business advantage to large companies with large databases over smaller businesses or start-ups. The purpose of this thesis is to find algorithms or methods that can be effective in solving some natural language processing problems on small databases. For our research, we built a content-based recommendation system. We tested similarity measures such as Latent Dirichlet Allocation, cosine similarity, long-short term memory neural network, and the RV coefficient. We also compared the efficiency of the term frequency-inverse document frequency versus the mutual information to give a weighting scheme for the cosine similarity. We also compared the effectiveness of mutual information versus using raw word count as thresholds to remove words from a dictionary for the other similarity measures. We also used external databases, one containing documents related to our problem and another having Wikipedia documents. We also used a pre-trained GLOVE word embedding vector for our neural networks and the RV coefficient. We concluded that the simplest algorithms generally work best when there is little data. We also proposed several possible solutions to improve the algorithms we tested